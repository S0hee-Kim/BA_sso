{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with Gensim\n",
    "\n",
    "pip install gensim\n",
    "pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "#메일 내용에서 hint가 되는 부분을 삭제 - 순수하게 내용만으로\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 library들을 import\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    min_length = 3 #최소 단어 크기\n",
    "    # nltk의 tokenizer를 이용해서 word 추출한 후에 소문자로 변환\n",
    "    words = map(lambda word: word.lower(), word_tokenize(text))\n",
    "    #words = word_tokenize(text.lower()) #이렇게 해도 되는지 확인\n",
    "    # stopwords 제외\n",
    "    words = [word for word in words if word not in cachedStopWords]\n",
    "    # portr stemmer 적용\n",
    "    tokens = (list(map(lambda token: PorterStemmer().stem(token),words)))\n",
    "    #tokens = [PorterStemmer().stem(token) for token in words]  #이렇게 해도 되는지 확인\n",
    "    #알파벳으로 이루어진 단어들만 추출\n",
    "    p = re.compile('[a-zA-Z]+');\n",
    "    filtered_tokens = list(filter (lambda token: p.match(token) and len(token) >= min_length,tokens))\n",
    "    #filtered_tokens = [token for token in tokens if p.match(token) and len(token) >= min_length]   #이렇게 해도 되는지 확인\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [tokenize(doc) for doc in newsgroups_train.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "['notic', 'save', 'model', 'map', 'plane', 'posit', 'care', 'file', 'reload', 'restart', 'given', 'default', 'posit', 'orient', 'save', 'file', 'positions/orient', 'preserv', 'anyon', 'know', 'inform', 'store', 'file', 'noth', 'explicitli', 'said', 'manual', 'save', 'textur', 'rule', 'file', 'like', 'abl', 'read', 'textur', 'rule', 'inform', 'anyon', 'format', 'file', 'file', 'format', 'avail', 'somewher', 'rych']\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[0])\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp36-cp36m-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\thgml\\anaconda3\\envs\\basemap\\lib\\site-packages (from gensim) (1.19.5)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\thgml\\anaconda3\\envs\\basemap\\lib\\site-packages (from gensim) (1.5.4)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Collecting Cython==0.29.23\n",
      "  Downloading Cython-0.29.23-cp36-cp36m-win_amd64.whl (1.6 MB)\n",
      "Installing collected packages: smart-open, dataclasses, Cython, gensim\n",
      "Successfully installed Cython-0.29.23 dataclasses-0.8 gensim-4.1.2 smart-open-5.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim으로 topic modeling을 하기 위해서는 Dictionary, Corpus, Model의 3단계를 거쳐야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in initital documents: 19661\n",
      "Number of unique words after removing rare and common words: 5628\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "print('Number of unique words in initital documents:', len(dictionary))\n",
    "\n",
    "# Filter out words that occur less than 10 documents, or more than 20% of the documents.\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.5)\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 5628\n",
      "Number of documents: 2034\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "# Set training parameters.\n",
    "num_topics = 25\n",
    "chunksize = 500 # size of the doc looked at every pass\n",
    "passes = 20 # number of passes through documents\n",
    "iterations = 40\n",
    "eval_every = 1  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Using cached pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "  Downloading pyLDAvis-3.3.0.tar.gz (1.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Note: you may need to restart the kernel to use updated packages.  Downloading pyLDAvis-3.2.2.tar.gz (1.7 MB)\n",
      "Requirement already satisfied: wheel>=0.23.0 in c:\\users\\thgml\\anaconda3\\envs\\basemap\\lib\\site-packages (from pyLDAvis) (0.37.0)\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\users\\thgml\\anaconda3\\envs\\basemap\\lib\\site-packages (from pyLDAvis) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.0 in c:\\users\\thgml\\anaconda3\\envs\\basemap\\lib\\site-packages (from pyLDAvis) (1.5.4)\n",
      "Requirement already satisfied: joblib>=0.8.4 in c:\\users\\thgml\\anaconda3\\envs\\basemap\\lib\\site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in c:\\users\\thgml\\anaconda3\\envs\\basemap\\lib\\site-packages (from pyLDAvis) (3.0.1)\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "Collecting funcy\n",
      "  Using cached funcy-1.16-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: pandas>=0.17.0 in c:\\users\\thgml\\anaconda3\\envs\\basemap\\lib\\site-packages (from pyLDAvis) (1.1.5)\n",
      "\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\thgml\\anaconda3\\envs\\basemap\\lib\\site-packages (from jinja2>=2.7.2->pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\thgml\\anaconda3\\envs\\basemap\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\thgml\\anaconda3\\envs\\basemap\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thgml\\anaconda3\\envs\\basemap\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.17.0->pyLDAvis) (1.16.0)\n",
      "Building wheels for collected packages: pyLDAvis, future\n",
      "  Building wheel for pyLDAvis (setup.py): started\n",
      "  Building wheel for pyLDAvis (setup.py): finished with status 'done'\n",
      "  Created wheel for pyLDAvis: filename=pyLDAvis-3.2.2-py2.py3-none-any.whl size=135594 sha256=2998ed91c44a47f4612797ffc16df01e498c10c8ec025e9c96a14d18999d893b\n",
      "  Stored in directory: c:\\users\\thgml\\appdata\\local\\pip\\cache\\wheels\\eb\\d1\\ac\\e8728b60e7d714da9ff6a52cb8659099c3be9e0dc19f522881\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=fcef6d8b556032bcb4dd67413dc4f5d0061ba88c3332b5a53cd6645262dd290e\n",
      "  Stored in directory: c:\\users\\thgml\\appdata\\local\\pip\\cache\\wheels\\6e\\9c\\ed\\4499c9865ac1002697793e0ae05ba6be33553d098f3347fb94\n",
      "Successfully built pyLDAvis future\n",
      "Installing collected packages: numexpr, future, funcy, pyLDAvis\n",
      "Successfully installed funcy-1.16 future-0.18.2 numexpr-2.7.3 pyLDAvis-3.2.2\n"
     ]
    }
   ],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pyldavis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "prepare() got an unexpected keyword argument 'sort'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-472f1eb9f808>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 이 오류는 도저히 해결이 안됩니다 ㅠㅠ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\basemap\\lib\\site-packages\\pyLDAvis\\gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \"\"\"\n\u001b[0;32m    123\u001b[0m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: prepare() got an unexpected keyword argument 'sort'"
     ]
    }
   ],
   "source": [
    "pyLDAvis.gensim.prepare(model, corpus, dictionary, sort=True)\n",
    "\n",
    "# 이 오류는 도저히 해결이 안됩니다 ㅠㅠ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
