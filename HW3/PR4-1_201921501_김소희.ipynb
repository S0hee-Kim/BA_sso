{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 마이닝\n",
    "---\n",
    "\n",
    "\n",
    "# 1. 텍스트 마이닝의 이해\n",
    "## 1.1. 텍스트 마이닝이란?\n",
    "* 텍스트로부터 양질의 정보를 뽑아내는 과정(위키피디아)\n",
    "* 양질의 정보는 통계적인 패턴 학습을 이용해 패턴이나 트렌드를 찾아내는 것에서 유래함.\n",
    "* 자연어 처리방법 등을 써서 텍스트를 데이터로 바꿔야하고, 바꾼 후에 패턴이나 트렌드를 찾아서 유용한 정보로 만들어내는 것이 텍스트 마이닝이다.\n",
    "\n",
    "\n",
    "## 1.2. 텍스트 마이닝의 이해를 위한 기본요구지식\n",
    "* 자연어 처리\n",
    "* 통계학 & 선형대수\n",
    "* 머신러닝\n",
    "* 딥러닝\n",
    "\n",
    "## 1.3. 텍스트 마이닝 방법\n",
    "* NLP(Natural Language Processing) 기본도구\n",
    "* 머신러닝(딥러닝) 등\n",
    "\n",
    "## 1.4. 텍스트 마이닝 단계\n",
    "1. 텍스트 마이닝의 대상은 Document\n",
    "2. Document 분석을 위해 단어단위로 쪼갬(tokenize), 변형되어 쓰여진 단어를 원형으로 바꿈(normalize)\n",
    "3. 정규화된 단어의 Sequence가 됨\n",
    "4. 이를 가지고 시퀀스 정보없이 벡터를 만들거나 시퀀스 정보를 가지고 벡터를 만들거나 시퀸스 정보를 가지고 워드 임베딩된 일련의 벡터로 만든다.\n",
    "5. 각각의 경우에 따라 사용하는 방법이 달라지게 된다.\n",
    "\n",
    "## 1.5. 텍스트 마이닝 적용분야\n",
    "* Document classification\n",
    "* Document generation\n",
    "* keyword extraction\n",
    "* topic modeling\n",
    "\n",
    "## 1.6. 텍스트 마이닝 도구 - 파이썬\n",
    "* NLTK - 가장 많이 알려진 NLP 라이브러리\n",
    "* Scikit Learn - 머신러닝 라이브러리, 기본적인 NLP, 다양한 텍스트 마이닝 관련 도구 지원\n",
    "* Gensim - Word2Vec으로 유명, sklearn과 마찬가지로 다양한 텍스트 관련 도구 지원\n",
    "* keras - RNN, seq2seq등 딥러닝 위주의 라이브러리 제공\n",
    "* pytorch\n",
    "***\n",
    "\n",
    "\n",
    "# 2. 텍스트 마이닝 방법론 \n",
    "## 2.1. 텍스트 마이닝 기본도구 - NLP\n",
    "* 목적 : document, sentence 등을 sparse vector로 변환\n",
    "* Tokenize - 대상이 되는 문서/ 문장을 최소 단위로 쪼갬\n",
    "* Text normalization - 최소 단위를 표준화\n",
    "* POS-tagging - 최소 의미단위로 나누어진 대상에 대해 품사를 부착\n",
    "* Chunking - POS-tagging의 결과를 명사구, 형용사구, 분사구 등과 같은 말모듬으로 다시 합치는 과정\n",
    "* BOW, TFIDF - tokenized 결과를 이용하여 문서를 vector로 표현\n",
    "\n",
    "## 2.2. Tokenize\n",
    "* Document를 Sentence의 집합으로 분리\n",
    "* Sentence를 word의 집합으로 분리\n",
    "* 의미없는 문자 등을 걸러 냄\n",
    "* 영어 vs 한글\n",
    "* 영어는 공백 기준으로 비교적 쉽게 tokenize 가능\n",
    "* 한글은 구조상 형태소 분석이 필요, 영어에 비해 어렵고 정확도 낮음\n",
    "\n",
    "## 2.3. Text Normalization\n",
    "* 동일한 의미의 단어가 다른 형태를 갖는 것을 보완\n",
    "* Stemming(어간 추출)\n",
    "    * 단수 - 복수, 현재형 - 미래형 등 단어의 다영한 변형을 하나로 통일\n",
    "    * 의미가 아닌 규칙에 의한 변환\n",
    "    * 영어의 경우, Porter stemmer, Lancaster stemmer 등이 유명\n",
    "* Lemmatization(표제어 추출)\n",
    "    * 사전을 이용하여 단어의 원형을 추출\n",
    "    * 품사를 고려\n",
    "    * 영어의 경우, 유명한 어휘 데이터베이스인 WordNet을 이용한 WordNet lemmatizer가 많이 쓰임\n",
    "    \n",
    "## 2.4. POS-tagging\n",
    "* 토큰화와 정규화 작업을 통해 나누어진 형태소에 대해 품사를 결정하여 할당하는 작업\n",
    "* 동일한 단어라도 문맥에 따라 의미가 달라지므로 품사를 알기 위해서는 문맥을 파악해야 함\n",
    "* text-to-speech에서도 각 단어에 대해 올바를 발음을 하기 위해 품사 태깅을 이용\n",
    "* 형태소 분석으로 번역되기도 하는데, 형태소 분석은 주어진 텍스트를 형태소 단위로 나누는 작업을 포함하므로 앞의 토큰화, 정규화 작업에 품사 태깅을 포함한 것으로 보는 것이 타당\n",
    " \n",
    "## 2.5. Chunking\n",
    "* Chunk는 말모듬을 뜻하며, 명사구, 형용사구, 분사구 등과 같이 주어와 동사가 없는 두 단어 이상의 집합인 구를 의미\n",
    "* Chunking은 주어진 텍스트에서 이와 같은 chunk를 찾는 과정\n",
    "* 즉, 형태소 분석의 결과인 각 형태소들을 서로 겹치지 않으면서 의미가 있는 구로 묶어나가는 과정임\n",
    "* 텍스트로부터 정보추출을 하기 위한 전 단계로 보거나 혹은 정보추출에 포함되기도 함\n",
    "\n",
    "## 2.6. 개체명 인식\n",
    "* 개체명은 기관, 단체, 사람, 날짜 등과 같이 특정 정보에 해당하는 명사구를 의미\n",
    "* 텍스트로부터 뭔가 의미 있는 정보를 추출하기 위한 방법으로 사용\n",
    "* 관계 인식\n",
    "    * NER에 의해 추출된 개체명들을 대상으로 그들 간의 관계를 추출하는 작업\n",
    "    * 특정 건물이 특정 장소에 위치하는 관계와 같은 지식을 텍스트로부터 추출할 때 사용\n",
    "    \n",
    "## 2.7. BOW(Bag of Words)\n",
    "* 시퀀스로 보지 않고 순서가 없다라고 가정하겠다.\n",
    "* Vector Space Model\n",
    "    * 문서를 bag of words로 표현\n",
    "    * 단어가 쓰여진 순서는 무시\n",
    "    * 모든 문서에 한 번 이상 나타난 단어들에 대해 유무로 문서를 표현\n",
    "* count vector\n",
    "    * 단어의 유무 대신 단어가 문서에 나타난 횟수로 표현\n",
    "    * count가 weight로 작용\n",
    "\n",
    "## 2.8. TFIDF\n",
    "* count vector의 문제점 - 많은 문서에 공통적으로 나타난 단어는 중요성이 떨어지는 단어일 가능성이 높음\n",
    "* 단어의 count를 단어가 나타난 문서의 수로 나눠서 자주 등장하지 않는 단어의 weight를 올림\n",
    "\n",
    "## 2.9. Text Classification\n",
    "* naive bayes - text categorization에 많이 쓰이는 인기있는 방법\n",
    "* 로지스틱 회귀분석\n",
    "    * 분류를 위한 회귀분석\n",
    "        * 종속 변수와 독립변수 간의 관계를 구체적인 함수로 나타내어 향후 예측 모델에 사용\n",
    "        * 종속 변수가 범주형 데이터를 대상으로 하며, 입력 데이터가 주어졌을 때 해당 데이터의 결과가 특정 분류로 나뉘기 때문에 일종의 분류 기법에 해당\n",
    "    * 텍스트 마이닝에서의 문제\n",
    "        * 추정해야 할 계수가 vector의 크기만큼 존재하므로, 과적합이 발생하기 쉽고 많은 데이터 셋이 필요\n",
    "        * 그럼에도 불구하고 잘 작동하는 편\n",
    "        * 정규화를 이용해 과적합 해결 노력\n",
    "* Ridge and Lasso Regression\n",
    "\n",
    "\n",
    "## 2. 10. 문서 분류의 활용 - Sentiment Analysis\n",
    "* 소비자의 감성과 관련된 텍스트 정보를 자동으로 추출하는 텍스트 마이닝 기술의 한 영역\n",
    "* 문서를 작성한 사람의 감정을 추출해 내는 기술로 문서의 주제보다 어떠한 감정을 가지고 있는가를 판단하여 분석한다. \n",
    "***\n",
    "\n",
    "\n",
    "# 3. 텍스트 마이닝의 문제\n",
    "## 3.1. Curse of Dimensionality\n",
    "* 차원의 저주 - 각 데이터 간의 거리가 너무 멀게 위치\n",
    "* 해결방법 : 더 많은 데이터, 차원 축소\n",
    "\n",
    "## 3.2. 단어 빈도의 불균형\n",
    "* Zipf's law(멱법칙) - 극히 소수의 데이터가 결정적인 영향을 미치게 됨\n",
    "* 해결방안 : feature selection, boolean BOW사용, log등의 함수를 이용해 weight 변경\n",
    "\n",
    "## 3.3. 단어가 쓰인 순서정보의 손실\n",
    "* 통계에 대한 의미 파악 vs 순서에 의한 의미 파악\n",
    "* Loss of sequence information\n",
    "    * 단어들의 순서 - context가 중요\n",
    "    * 특히 번역과 같은 sequence to sequence 문제에서 매우 중요\n",
    "* 해결방안\n",
    "    * n-gram : 부분적 해결, 주로 classification 문제에서 유용\n",
    "    * Deep Learning : RNN, Attention, Transformer, BERT\n",
    "***\n",
    "\n",
    "\n",
    "# 4. 문제의 해결방안 - 기존의 방법\n",
    "## 4.1. Dimensionality Reduction(차원축소)\n",
    "* 차원의 저주를 해결하기 \n",
    "* Feature selection\n",
    "* Feature extraction\n",
    "* Embedding\n",
    "* Deep Learning\n",
    "\n",
    "### 4.1.1. feature extraction\n",
    "* 첫번째 방법은 PCA(주성분 분석)\n",
    "    * 선형결합, 공분산행렬\n",
    "    * 고유값이 큰 순서대로 고유벡터를 정렬하여 차원 선택\\\n",
    "    * 선택된 고유벡터와 x의 선형결합으로 차원 축소\n",
    "    \n",
    "## 4.2. LSA(Latent Semantic Analysis)\n",
    "* SVD(특이값 분해)이라는 방법을 사용\n",
    "\n",
    "## 4.3. 잠재의미분석\n",
    "* 활용) 문서 간의 유사도, 단어 간의 유사도 파악\n",
    "\n",
    "## 4.4. Topic Modeling\n",
    "* 문서가 굉장히 많이 있을 때 분석하는 방법 중 하나\n",
    "* topic이 있고, 주제를 구성하는 단어들이 있음\n",
    "* 토픽을 구성하는 단어를 조합해서 문서를 쓰는 방법\n",
    "* LDA 알고리즘\n",
    "    * 원리) 토픽은 주제를 의미하는 용어로 사용되며, 각 문서들이 특정한 주제에 속할 확률분포와 주제로부터 특정 단어들이 파생되어 나올 확률분포가 주어졌을 때, 이 두 확률분포를 조합하여 각 문서들에 들어가는 단어들의 확률분포를 계산\n",
    "\n",
    "## 4.5. Word Embedding\n",
    "* 텍스트에서 사용되는 단어들을 어떻게 컴퓨터 안에서 표현하고 담을 것이냐에 대한 문제\n",
    "* 단어에 대한 vector의 차원축소가 목표\n",
    "* 단어의 표현\n",
    "    * Term-Document Matrix에서 Document별 count vector - 일반화가 어려움\n",
    "    * one-hot-encoding - extremely sparse\n",
    "* 순서)\n",
    "    1. one-hot-encoding으로 표현된 단어를 dense vector로 변환\n",
    "    2. 변환된 vector를 이용하여 학습\n",
    "    3. 최종목적에 맞게 학습에 의해 vector가 결정됨\n",
    "    4. 학습목적 관점에서의 단어의 의미를 내포\n",
    "* Word Embedding 을 이용한 문서 분류\n",
    "    * BOW와는 다른 관점의 문서 표현\n",
    "    * 단순한 분류모형\n",
    "\n",
    "## 4.6. Word2Vec\n",
    "* 문장에 나타난 단어들의 순서를 이용해 word embedding을 수행\n",
    "* 학습방법\n",
    "    * sliding window를 이용한 학습 set구성\n",
    "    * embedding vector\n",
    "* 의미\n",
    "    * 단어의 위치에 기반하여 의미를 내포하는 vector 생성\n",
    "        * 비슷한 위치에 나타나는 단어들은 비슷한 vector를 가지게 됨\n",
    "        * 단어 간의 유사성을 이용하여 연산이 가능\n",
    "        \n",
    "## 4.7. ELMo(Embeddings from Language Model)\n",
    "* 사전 훈련된 언어 모델을 사용하는 워드 임베딩 방법론\n",
    "* 동일한 단어가 문맥에 따라 전혀 다른 의미를 가지는 것을 반영하기 위해 개발됨\n",
    "\n",
    "## 4.8. Transfer Learning\n",
    "* 텍스트 마이닝에서의 전이학습\n",
    "    * 단어에 대해 학습되어 있는 dense vector를  그대로 가져다 씀\n",
    "    \n",
    "## 4.9. Document Embedding\n",
    "* Word2Vec은 word에 대해 dense vector를 생성하지만, document vector는 여전히 sparse\n",
    "***\n",
    "\n",
    "\n",
    "# 5. 딥러닝에 의한 방법\n",
    "## 5.1 RBM\n",
    "* 딥러닝의 문을 열어준 기법임\n",
    "* 사전학습 목적으로 개발 \n",
    "    * 차원을 변경하면서 원래의 정보량 유지가 목적\n",
    "* Deep NIN의 vanishing gradient 문제 해결을 위해 제안\n",
    "* 사전학습을 통한 차원 축소에 사용 가능\n",
    "\n",
    "## 5.2. Autoencoder\n",
    "* RBM과 유사한 개념\n",
    "    * encoder로 차원을 축소하고 decorder로 다시 복원했을 때, 원래의 x와 복원한 x' 이 최대한 동일하도록 학습\n",
    "* 작동방식은 PCA와 유사\n",
    "    * 데이터에 내재된 일정한 구조-연관성을 추출\n",
    "    \n",
    "## 5.3. Context(sequence)의 파악\n",
    "* N-gram\n",
    "    * 문맥을 파악하기 위한 전통적 방법\n",
    "    * 대상이 되는 문자열을 하나의 단어 단위가 아닌, 두 개 이상의 단위로 잘라서 처리\n",
    "    * bi-gram, tri-gram...\n",
    "    * 보통 unigram에 bi-gram, tri-gram을 추가하면서 feature의 수를 증가시켜 사용\n",
    "    * 문맥 파악에 유리하나, dimension이 더욱 증가\n",
    "* 딥러닝과 텍스트마이닝 - RNN\n",
    "    * 문장을 단어들의 sequence 혹은 series로 처리(순서가 있음)\n",
    "    * 뒷 단어에 대한 hidden node가 앞 단어의 hidden node 값에도 영향을 받도록 함\n",
    "    * 그 외에도 단어들 간의 관계를 학습할 수 있는 모형을 고안\n",
    "\n",
    "## 5.4. LSTM\n",
    "* RNN의 문제) 문장이 길수록 층이 깊은 형태를 갖게 됨 -> 경사가 소실되는 문제 발생 -> 앞부분의 단어 정보가 학습되지 않음\n",
    "* 직통 통로를 만들어 RNN의 문제를 해결 \n",
    "\n",
    "## 5.5 Bi-LSTM\n",
    "* 단방향 LSTM의 문제\n",
    "    * 단어 순서가 갖는 문맥 정보가 한 방향으로만 학습된다.\n",
    "    * 사진의 뒤에 오는 단어에 의해 영향을 받는 경우, 학습이 되지 않음\n",
    "* Bi-LSTM\n",
    "    * 양방향으로 LSTM을 구성하여 두 결과를 합침\n",
    "    * 양방향 순서를 모두 학습\n",
    "   \n",
    "## 5.6. 합성곱 신경망(CNN)\n",
    "* 원래 이미지 처리를 위해 개발된 신경망으로, 현재는 인간의 이미지 인식보다 더 나은 인식 성능을 보이고 있음\n",
    "* 주변 정보를 학습한다는 점을 이용하여 텍스트의 문맥을 학습하는 연구를 진행하였고 뛰어난 성능을 보이게 되면서 자연어 처리에서의 활용분야가 넓어지게 됨.\n",
    "* 1차원 CNN모형을 사용함(텍스트이기 때문에)\n",
    "\n",
    "## 5.7. Sequence-to-sequence\n",
    "* 지금까지는 입력은 sequence, 출력은 하나의 값이 경우가 일반적이었으나 번역, chat-bot, summarize 등은 출력도 sequence가 되어야 함.\n",
    "\n",
    "## 5.8. Attention\n",
    "* 출력에 나온 어떤 단어는 입력에 있는 특정 단어들에 민감한 것에 착안\n",
    "* 입력의 단어들로부터 출력 단어에 직접 링크를 만듦\n",
    "\n",
    "## 5.9. Transformer\n",
    "* 입력 단어들끼리도 상호연관성이 있는 것에 착안\n",
    "* 입력 -> 출력으로의 attention외에 입력 단어들 간의 attention, 입력 + 출력 -> 출력으로의 attention을 추가\n",
    "* encoder와 decoder가 서로 다른 attention 구조를 사용\n",
    "* RNN이 사라지고 self-attention이 이를 대신\n",
    "\n",
    "## 5.10. BERT\n",
    "* 양방향 transformer 인코더를 사용\n",
    "* transfoer learning에서 featuer+model을 함께 transfer하고 fine tuning을 통해서 적용하는 방식을 선택\n",
    "* 거의 모든 분야에서 top score를 기록\n",
    "* 다양한 text mining task에 전이학습을 이용해 적용가능한 구조를 제안"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
